<!DOCTYPE html>
<html lang="en" >

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:url" content="https://dtolpin.github.io/2022/09/how-data-scientists-fail/">
  <meta property="og:site_name" content="Offtopia — nothing personal">
  <meta property="og:title" content="How Data Scientists Fail">
  <meta property="og:description" content="I am going to job interviews, again. This time, a frequent request is: “Tell us about a failed project”. Of course, I never fail as a data scientist, how could I? A data science task involves a combination of domain knowledge and data, neither is held or produced by me, and a question someone else wants an answer to. All I do as a data scientist is encoding the domain knowledge as a model, updating the model’s latent variables based on the data, and computing a quantitative answer to the question. There are ways to ensure adequacy of the model, check convergence of inference, and express uncertainty of the answer. Just doing all these steps by the book ensures that there is absolutely no way to fail. Consider the task of classifying hand-written digits — although different models may have different accuracy, there is no way to ‘fail’ as long as one does things as taught. Or is there?">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-09-04T16:09:33+03:00">
    <meta property="article:modified_time" content="2022-09-04T16:09:33+03:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="How Data Scientists Fail">
  <meta name="twitter:description" content="I am going to job interviews, again. This time, a frequent request is: “Tell us about a failed project”. Of course, I never fail as a data scientist, how could I? A data science task involves a combination of domain knowledge and data, neither is held or produced by me, and a question someone else wants an answer to. All I do as a data scientist is encoding the domain knowledge as a model, updating the model’s latent variables based on the data, and computing a quantitative answer to the question. There are ways to ensure adequacy of the model, check convergence of inference, and express uncertainty of the answer. Just doing all these steps by the book ensures that there is absolutely no way to fail. Consider the task of classifying hand-written digits — although different models may have different accuracy, there is no way to ‘fail’ as long as one does things as taught. Or is there?">
<meta name="generator" content="Hugo 0.152.2">


    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "How Data Scientists Fail",
  "url": "https://dtolpin.github.io/2022/09/how-data-scientists-fail/",
  "wordCount": "609",
  "datePublished": "2022-09-04T16:09:33+03:00",
  "dateModified": "2022-09-04T16:09:33+03:00",
  "author": {
    "@type": "Person",
    "name": "David Tolpin"
  }
}
</script>



    <link rel="canonical" href="https://dtolpin.github.io/2022/09/how-data-scientists-fail/">

    <title>How Data Scientists Fail | Offtopia — nothing personal</title>

    
    <!-- combined, minified CSS -->
    
    <link href="https://dtolpin.github.io/css/style.c6ba80bc50669557645abe05f86b73cc5af84408ed20f1551a267bc19ece8228.css" rel="stylesheet" integrity="sha256-xrqAvFBmlVdkWr4F&#43;GtzzFr4RAjtIPFVGiZ7wZ7Ogig=" crossorigin="anonymous">
    
    
    <link href="https://dtolpin.github.io/css/substyle.f6bbec6fb714a00539d34a74ab7edb3a54048cb7745882eea95ba078edf0803c.css" rel="stylesheet" integrity="sha256-9rvsb7cUoAU500p0q37bOlQEjLd0WILuqVugeO3wgDw=" crossorigin="anonymous">

    <!-- minified Font Awesome for SVG icons -->
    
    <script defer src="https://dtolpin.github.io/js/fontawesome.min.f5072c55a0721857184db93a50561d7dc13975b4de2e19db7f81eb5f3fa57270.js" integrity="sha256-9QcsVaByGFcYTbk6UFYdfcE5dbTeLhnbf4HrXz&#43;lcnA=" crossorigin="anonymous"></script>

    <!-- RSS 2.0 feed -->
    

    

  </head>

  <body>

    
    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link " href="https://dtolpin.github.io/">Home</a>
          
          <a class="nav-link" href="/about/" title="">About</a>
          
          
          <a class="nav-link" href="/academic/" title="">Academic</a>
          
        </nav>
      </div>
    </div>
    

    
    
    <header class="blog-header">
      <div class="container">
        <h1 class="blog-title" dir="auto"><a href="https://dtolpin.github.io/" rel="home">Offtopia — nothing personal</a></h1>
        
      </div>
    </header>
    
    

    
    <div class="container">
      <div class="row">
        <div class="col-sm-8 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title" dir="auto"><a href="https://dtolpin.github.io/2022/09/how-data-scientists-fail/">How Data Scientists Fail</a></h2>
    
    <h3 class="blog-post-subtitle">What can go wrong in a data science task</h3>
    
    <p class="blog-post-meta">
<time datetime="2022-09-04T16:09:33+03:00">Sun Sep 04, 2022</time>
</p>
  </header>
  <p>I am going to job interviews, again. This time, a frequent
request is: &ldquo;Tell us about a failed project&rdquo;. Of course, I never
fail as a data scientist, how could I? A data science task
involves a combination of domain knowledge and data, neither is
held or produced by me, and a question someone else wants an
answer to. All I do as a data scientist is encoding the domain
knowledge as a model, updating the model&rsquo;s latent variables
based on the data, and computing a quantitative answer to the
question. There are ways to <a href="http://www.stat.columbia.edu/~gelman/book/">ensure adequacy of the model, check
convergence of inference, and express uncertainty of the
answer</a>.
Just doing all these steps by the book ensures that there is
absolutely no way to fail. Consider the task of classifying
<a href="http://yann.lecun.com/exdb/mnist/">hand-written digits</a> &mdash;
although different models may have different accuracy, there is
no way to ‘fail’ as long as one does things as taught. Or is
there?</p>
<p>Let us see what a failure is. A failure is not a wrong model
choice, or poor convergence of inference, or a mistake in
computing compatibility intervals. Those are manifestations of
incompetence rather than failure. A failure happens when the
data scientist does everything right, but still causes a
disaster, hopefully small and easy to recover from. Let me
argue that a failure can only happen if the data scientist makes
a decision based on hard to validate assumptions, and those
assumptions turn out to be too far from the reality.</p>
<p>But do data scientists make any ‘voluntary’ decisions at all?
Turns out they do. If the task is label assignment, then the
decision is the compromise between precision and recall. For
forecasting, the compromise is between forecast stability and
width of confidence interval. For clustering, one has to
balance, explicitly or implicitly, between the number of
clusters and similarity of members of each cluster. Despite
apparent dissimilarities, all of these decisions are kinds of
<a href="https://towardsdatascience.com/the-exploration-exploitation-dilemma-f5622fbe1e82"><em>exploration-exploitation
compromise</em></a>.  Exploration-exploitation
compromise always addresses yet unseen data and yet undiscovered
knowledge, and thus acting by the book does not guarantee
success. Sometimes, a wrong exploration-exploitation compromise
is made, and this is how a data science project fails.</p>
<p>To conclude, an example. I took upon a task of automated traffic
acquisition &mdash; paying for visits to a web page to earn from
advertisements on that page. Visitors are acquired through an
auction, so one wants to bid higher if one anticipates higher
earnings. I deployed a model for temporal forecasting of visit
value, and a decision algorithm to choose the optimal bid given
the forecast. The algorithm accounted for forecast
uncertainty, maximized expected gain, took care of risks, and
did everything ‘right’, by the book. It worked well for a while,
but eventually &mdash; and suddenly &mdash; two different extreme cases
popped up, incurring losses (which luckily where quickly
mitigated):</p>
<ul>
<li>On a small number of campaigns, the actual visit value
suddenly dropped after steadily going up, violating smoothness
assumptions. A smoothness assumption is over-exploitation. The
result was trading at loss, for a short time but with high
traffic volume and cost.</li>
<li>On another small group of campaigns, the traffic went down
almost to zero due to a low visit value forecast, followed by
low bids due to a broad safety margin. A broad safety margin
is over-exploration, but with close to zero traffic
the forecasting ceased to be reliable, resulting in wasted
resources and lost opportunities.</li>
</ul>
<p>Both failures were fixed, eventually. What is important though
is the cause of the failures: both happened due to inadequate
exploration-exploitation assumptions introduced into the
algorithm, neither could be discovered based on either
historical data or model-based simulations.</p>

  

  
  <hr>
  <footer>

  
    <section>
    <h4>Share</h4>
    <nav class="nav sharing-icons">
      <a class="nav-item" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fdtolpin.github.io%2f2022%2f09%2fhow-data-scientists-fail%2f" title="Share on Facebook"><span class="fab fa-facebook-f fa-2x" aria-hidden="true"></span></a>
      <a class="nav-item" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdtolpin.github.io%2f2022%2f09%2fhow-data-scientists-fail%2f" title="Share on LinkedIn"><span class="fab fa-linkedin-in fa-2x" aria-hidden="true"></span></a>
      <a class="nav-item" href="https://twitter.com/intent/tweet?url=https%3a%2f%2fdtolpin.github.io%2f2022%2f09%2fhow-data-scientists-fail%2f&amp;text=How%20Data%20Scientists%20Fail" title="Tweet this"><span class="fab fa-twitter fa-2x"></span></a>
    </nav>
  </section>

  

  
  </footer>
  

</article> 



        </div> <!-- /.blog-main -->

        <aside class="col-sm-3 ml-auto blog-sidebar">
  

  
        <section class="sidebar-module">
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">


<li><a href="/2022/09/how-data-scientists-fail/">How Data Scientists Fail</a></li>

<li><a href="/2021/08/double-speed-replay/">Double Speed Replay</a></li>

<li><a href="/2021/08/how-to-train-your-program/">How To Train Your Program</a></li>

<li><a href="/2021/08/stochastic-conditioning/">Stochastic conditioning</a></li>

<li><a href="/2020/08/bda-model-too-tough-for-stan/">BDA Model Too Tough for Stan</a></li>

    </ol>
  </section>

  

  
  <section class="sidebar-module">
    <h4>Links</h4>
    <ol class="list-unstyled">
      
      <li><a href="https://infergo.org/">Infergo</a></li>
      
      <li><a href="https://offtopia.net/wp/">Old blog</a></li>
      
    </ol>
  </section>
  
</aside>


      </div> <!-- /.row -->
    </div> <!-- /.container -->
    

    
    <footer class="blog-footer">
      <p dir="auto">
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
      <p>
      <a href="#">Back to top</a>
      </p>
    </footer>
    

    
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>

  </body>
</html>
