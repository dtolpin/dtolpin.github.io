<!DOCTYPE html>
<html lang="en" >

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:url" content="https://dtolpin.github.io/2021/08/stochastic-conditioning/">
  <meta property="og:site_name" content="Offtopia — nothing personal">
  <meta property="og:title" content="Stochastic conditioning">
  <meta property="og:description" content="Probabilistic programs implement statistical models. Commonly, probabilistic programs follow the Bayesian generative pattern:
\begin{equation} \begin{aligned} x &amp; \sim \mathrm{Prior} \\ y &amp; \sim \mathrm{Conditional}(x) \end{aligned} \end{equation}
A prior is imposed on the latent variable $x$. Then, observations $y$ are drawn from a distribution conditioned on $x$. The program and the observations are passed to an inference algorithm which infers the posterior of latent variable $x$.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-08-09T15:17:00+03:00">
    <meta property="article:modified_time" content="2021-08-09T15:17:00+03:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Stochastic conditioning">
  <meta name="twitter:description" content="Probabilistic programs implement statistical models. Commonly, probabilistic programs follow the Bayesian generative pattern:
\begin{equation} \begin{aligned} x &amp; \sim \mathrm{Prior} \\ y &amp; \sim \mathrm{Conditional}(x) \end{aligned} \end{equation}
A prior is imposed on the latent variable $x$. Then, observations $y$ are drawn from a distribution conditioned on $x$. The program and the observations are passed to an inference algorithm which infers the posterior of latent variable $x$.">
<meta name="generator" content="Hugo 0.152.2">


    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Stochastic conditioning",
  "url": "https://dtolpin.github.io/2021/08/stochastic-conditioning/",
  "wordCount": "814",
  "datePublished": "2021-08-09T15:17:00+03:00",
  "dateModified": "2021-08-09T15:17:00+03:00",
  "author": {
    "@type": "Person",
    "name": "David Tolpin"
  }
}
</script>



    <link rel="canonical" href="https://dtolpin.github.io/2021/08/stochastic-conditioning/">

    <title>Stochastic conditioning | Offtopia — nothing personal</title>

    
    <!-- combined, minified CSS -->
    
    <link href="https://dtolpin.github.io/css/style.c6ba80bc50669557645abe05f86b73cc5af84408ed20f1551a267bc19ece8228.css" rel="stylesheet" integrity="sha256-xrqAvFBmlVdkWr4F&#43;GtzzFr4RAjtIPFVGiZ7wZ7Ogig=" crossorigin="anonymous">
    
    
    <link href="https://dtolpin.github.io/css/substyle.f6bbec6fb714a00539d34a74ab7edb3a54048cb7745882eea95ba078edf0803c.css" rel="stylesheet" integrity="sha256-9rvsb7cUoAU500p0q37bOlQEjLd0WILuqVugeO3wgDw=" crossorigin="anonymous">

    <!-- minified Font Awesome for SVG icons -->
    
    <script defer src="https://dtolpin.github.io/js/fontawesome.min.f5072c55a0721857184db93a50561d7dc13975b4de2e19db7f81eb5f3fa57270.js" integrity="sha256-9QcsVaByGFcYTbk6UFYdfcE5dbTeLhnbf4HrXz&#43;lcnA=" crossorigin="anonymous"></script>

    <!-- RSS 2.0 feed -->
    

    

  </head>

  <body>

    
    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link " href="https://dtolpin.github.io/">Home</a>
          
          <a class="nav-link" href="/about/" title="">About</a>
          
          
          <a class="nav-link" href="/academic/" title="">Academic</a>
          
        </nav>
      </div>
    </div>
    

    
    
    <header class="blog-header">
      <div class="container">
        <h1 class="blog-title" dir="auto"><a href="https://dtolpin.github.io/" rel="home">Offtopia — nothing personal</a></h1>
        
      </div>
    </header>
    
    

    
    <div class="container">
      <div class="row">
        <div class="col-sm-8 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title" dir="auto"><a href="https://dtolpin.github.io/2021/08/stochastic-conditioning/">Stochastic conditioning</a></h2>
    
    <h3 class="blog-post-subtitle"><a href="https://icml.cc/virtual/2021/spotlight/8608">https://icml.cc/virtual/2021/spotlight/8608</a></h3>
    
    <p class="blog-post-meta">
<time datetime="2021-08-09T15:17:00+03:00">Mon Aug 09, 2021</time>
</p>
  </header>
  <p>Probabilistic programs implement statistical models. Commonly,
probabilistic programs follow the Bayesian generative pattern:</p>
<p>\begin{equation}
\begin{aligned}
x &amp; \sim \mathrm{Prior} \\
y &amp; \sim \mathrm{Conditional}(x)
\end{aligned}
\end{equation}</p>
<ul>
<li>A prior is imposed on the latent variable $x$.</li>
<li>Then, observations $y$ are drawn from a distribution conditioned
on $x$.</li>
</ul>
<p>The program and the observations are passed to an inference
algorithm which infers the posterior of latent variable $x$.</p>
<p>The questions is: what is observed?</p>
<p>In a basic, familiar, setting, samples from the joint data
distribution are observed. However, it is also possible that the
observations are</p>
<ul>
<li>samples from marginal distributions;</li>
<li>summary statistics;</li>
<li>and even the data distribution as a lazy infinite sampler.</li>
</ul>
<p>These cases frequently arise in real-life scenarios. Consider,
for example</p>
<ul>
<li>
<p>an <strong>anonymized clinical study in a hospital</strong>, where patients
bearing a particular disease are monitored, but only summary
statistics of each symptom are reported, to preserve patients'
privacy;</p>
</li>
<li>
<p>or the famous <strong>‘canadian traveller’ problem</strong>, where the traveller
plans a road trip, but some roads can be closed due to bad
weather, so the model must be conditioned on the distribution
of possible road closure combinations.</p>
</li>
</ul>
<p>Existing probabilistic programming frameworks cannot represent
such cases straighforwardly.</p>
<p>To address this problem, we propose to generalize deterministic
conditioning $x|y=y_0$ on values to stochastic conditioning $x|y
\sim D_0$ on distributions.</p>
<p>Formally, a probabilistic program with stochastic conditioning
is a tuple $(p(x, y), D)$, consisting of</p>
<ul>
<li>the computation $p(x, y) = p(x)p(y|x)$ of joint probability of assignments to x and y,</li>
<li>and the observed distribution of D, with density $q(y)$.</li>
</ul>
<p>We aim to infer $p(x|y \sim D)$, the distribution of x given y
distributed as D. For that we need (unnormalized)</p>
<p>$$p(x, y \sim D) = p(x)p(y \sim D|x)$$</p>
<p>However, the model computes the joint probability $p(x, y)$
of x and y rather than of x and D. We need to express the
conditional probability of D given x in terms of y given x, and
D.  Thus, we define the former in terms of the latter:</p>
<p>$$p(y \sim D|x) \propto \exp \left( \int_Y (\log p(y|x)),q(y)dy \right)=\prod\nolimits_Y p(y|x)^{q(y)dy}$$</p>
<blockquote>
<p>The probability of y distributed as D given x is the integral
of log probability of y given x over D, exponentiated.
Intuitively, this can be interperted as the product
probability of all values of y given x, according to their
observation probabilities.</p>
</blockquote>
<p>Inference algorithms for deterministic conditioning cannot be
directly applied to programs with stochastic conditioning.
In principle, nested Monte Carlo estimation can be used,
but it is slow and cumbersome to code.</p>
<p>However, with our definition of conditional probability an
unbiased Monte Carlo estimate of log probability is available:</p>
<p>$$\log p(y \sim D|x) \approx \frac 1 N \sum\nolimits_{i=1}^N \log p(y_i|x)$$</p>
<p>This estimate is similar to the estimate used for subsampling
of tall data. So, submsampling algorithms, such as</p>
<ul>
<li>pseudo-marginal MH;</li>
<li>stochastic gradient MCMC;</li>
<li>stochastic VI</li>
</ul>
<p>can be used, with little modification.</p>
<p>We demonstrate stochastic conditioning in action on a case study
based on Donald Rubin&rsquo;s paper from 1983. The task is to estimate
the total population fo New York state (804 municipalities)
based on a sample of 100 municipalities (two samples are
considered):</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Population (N=804)</th>
          <th>Sample 1 (n=100)</th>
          <th>Sample 2 (n=100)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>total</td>
          <td>13,776,663</td>
          <td>1,966,745</td>
          <td>3,850,502</td>
      </tr>
      <tr>
          <td>mean</td>
          <td>17,135</td>
          <td>19,667</td>
          <td>38,505</td>
      </tr>
      <tr>
          <td>sd</td>
          <td>139,147</td>
          <td>142,218</td>
          <td>228,625</td>
      </tr>
      <tr>
          <td>lowest</td>
          <td>19</td>
          <td>164</td>
          <td>162</td>
      </tr>
      <tr>
          <td>5%</td>
          <td>336</td>
          <td>308</td>
          <td>315</td>
      </tr>
      <tr>
          <td>25%</td>
          <td>800</td>
          <td>891</td>
          <td>863</td>
      </tr>
      <tr>
          <td>median</td>
          <td>1,668</td>
          <td>2,081</td>
          <td>1,740</td>
      </tr>
      <tr>
          <td>75%</td>
          <td>5,050</td>
          <td>6,049</td>
          <td>5,239</td>
      </tr>
      <tr>
          <td>95%</td>
          <td>30,295</td>
          <td>25,130</td>
          <td>41,718</td>
      </tr>
      <tr>
          <td>highest</td>
          <td>2,627,319</td>
          <td>1,424,815</td>
          <td>1809578</td>
      </tr>
  </tbody>
</table>
<blockquote>
</blockquote>
<p>The original study had access to populations of each
of the 100 municipalities in the sample, but the paper reports
only the summary statistics &mdash; the mean, the standard deviation,
and the quantiles. Can we still perform inference?</p>
<p>It turns out we can, we stochastic conditioning, and here is the
model:</p>
<p>\begin{align}
&amp; y_{1\ldots n} \sim \mathit{Quantiles} \\
&amp; &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; \\
&amp; m \sim \mathrm{Normal}\left(\mathit{mean}, {\mathit{sd}}/ {\sqrt n}\right) \\
&amp;\log s^2\sim\mathrm{Uniform}(-\infty,\infty) \\
&amp; \\
&amp; \sigma = \sqrt{\log \left(s^2/m^2 + 1\right)} \\
&amp; \mu  = \log m - {\sigma^2} / 2 \\
&amp; \\
&amp; y_{1\ldots n}\vert m,s^2 \sim  \mathrm{LogNormal}(\mu, \sigma)
\end{align}</p>
<p>Below the rule,
we define a model as though the populations of each municipality
were observed. We impose a prior on the parameters, and then
observe the populations from log-Normal distribution. However,
instead of passing fixed observations of populations, we
observe, through samples, a piecewise uniform quantile
distribution (above the rule). The model is differentiable, so
we can use  stochastic gradient Markov Chain Monte Carlo for
inference.</p>
<p><img src="/images/stochastic-conditioning/nypopu-estimate.svg" alt="Posterior"></p>
<p>In the posterior, the 95% compatibility intervals (solid
rectangles) for each of the two samples cover the true total
(red vertical line) and are even tighter than reported by Rubin
(using power-transformed normal), despite being based on less
information.</p>
<p><a href="http://proceedings.mlr.press/v139/tolpin21a.html">The paper</a>
gives rigorous theoretical threatment of stochastic
conditioning, along with intuitive explanations on didactic
examples, and several elaborated case studies. The case studies
are implemented using Infergo, a probabilistic programming
framework for Go, and are available in a public <a href="https://bitbucket.org/dtolpin/stochastic-conditioning">git
repository</a>.</p>


  

  
  <hr>
  <footer>

  
    <section>
    <h4>Share</h4>
    <nav class="nav sharing-icons">
      <a class="nav-item" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fdtolpin.github.io%2f2021%2f08%2fstochastic-conditioning%2f" title="Share on Facebook"><span class="fab fa-facebook-f fa-2x" aria-hidden="true"></span></a>
      <a class="nav-item" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdtolpin.github.io%2f2021%2f08%2fstochastic-conditioning%2f" title="Share on LinkedIn"><span class="fab fa-linkedin-in fa-2x" aria-hidden="true"></span></a>
      <a class="nav-item" href="https://twitter.com/intent/tweet?url=https%3a%2f%2fdtolpin.github.io%2f2021%2f08%2fstochastic-conditioning%2f&amp;text=Stochastic%20conditioning" title="Tweet this"><span class="fab fa-twitter fa-2x"></span></a>
    </nav>
  </section>

  

  
  </footer>
  

</article> 



        </div> <!-- /.blog-main -->

        <aside class="col-sm-3 ml-auto blog-sidebar">
  

  
        <section class="sidebar-module">
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">


<li><a href="/2022/09/how-data-scientists-fail/">How Data Scientists Fail</a></li>

<li><a href="/2021/08/double-speed-replay/">Double Speed Replay</a></li>

<li><a href="/2021/08/how-to-train-your-program/">How To Train Your Program</a></li>

<li><a href="/2021/08/stochastic-conditioning/">Stochastic conditioning</a></li>

<li><a href="/2020/08/bda-model-too-tough-for-stan/">BDA Model Too Tough for Stan</a></li>

    </ol>
  </section>

  

  
  <section class="sidebar-module">
    <h4>Links</h4>
    <ol class="list-unstyled">
      
      <li><a href="https://infergo.org/">Infergo</a></li>
      
      <li><a href="http://offtopia.net/wp/">Old blog</a></li>
      
    </ol>
  </section>
  
</aside>


      </div> <!-- /.row -->
    </div> <!-- /.container -->
    

    
    <footer class="blog-footer">
      <p dir="auto">
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
      <p>
      <a href="#">Back to top</a>
      </p>
    </footer>
    

    
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>

  </body>
</html>
