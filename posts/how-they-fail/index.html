<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="How Data Scientists Fail" />
<meta property="og:description" content="I am going to job interviews, again. This time, a frequent
request is: &ldquo;Tell us about a failed project&rdquo;. Of course, I never
fail as a data scientist, how could I? A data science task
involves a combination of domain knowledge and data, neither is
held or produced by me, and a question someone else wants an
answer to. All I do as a data scientist is encoding the domain
knowledge as a model, updating the model&rsquo;s latent variables
based on the data, and computing a quantitative answer to the
question. There are ways to ensure adequacy of the model, check
convergence of inference, and express uncertainty of the
answer.
Just doing all these steps by the book ensures that there is
absolutely no way to fail. Consider the task of classifying
hand-written digits &mdash;
although different models may have different accuracy, there is
no way to ‘fail’ as long as one does things as taught. Or is
there?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://dtolpin.github.io/posts/how-they-fail/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-04T16:09:33+03:00" />
<meta property="article:modified_time" content="2022-09-04T16:09:33+03:00" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="How Data Scientists Fail"/>
<meta name="twitter:description" content="I am going to job interviews, again. This time, a frequent
request is: &ldquo;Tell us about a failed project&rdquo;. Of course, I never
fail as a data scientist, how could I? A data science task
involves a combination of domain knowledge and data, neither is
held or produced by me, and a question someone else wants an
answer to. All I do as a data scientist is encoding the domain
knowledge as a model, updating the model&rsquo;s latent variables
based on the data, and computing a quantitative answer to the
question. There are ways to ensure adequacy of the model, check
convergence of inference, and express uncertainty of the
answer.
Just doing all these steps by the book ensures that there is
absolutely no way to fail. Consider the task of classifying
hand-written digits &mdash;
although different models may have different accuracy, there is
no way to ‘fail’ as long as one does things as taught. Or is
there?"/>



    <link rel="canonical" href="http://dtolpin.github.io/posts/how-they-fail/">

    <title>
      
        How Data Scientists Fail | Offtopia — nothing personal
      
    </title>

    
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">

    <link href="http://dtolpin.github.io/css/style.css" rel="stylesheet">

	

    

    
  </head>
  <body>
    
      <header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <a class="navbar-brand" href="/">
            Offtopia — nothing personal
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
            aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
            <ul class="navbar-nav">
                
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/about/">About</a>
                    
                </li>
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/academic/">Academic</a>
                    
                </li>
                
            </ul>
            
        </div>
    </nav>
</header>
    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main">

          

<header>
    <h2 class="blog-post-title">
        <a class="text-dark" href="/posts/how-they-fail/">How Data Scientists Fail</a>
    </h2>
    
    <h3 class="blog-post-subtitle">What can go wrong in a data science task</h3>
    
    


<div class="blog-post-date text-secondary">
    
        Sep 4, 2022
    
    
        by <span rel="author">David Tolpin</span>
    
</div>

    
    
    <hr>
</header>
<article class="blog-post">
    <p>I am going to job interviews, again. This time, a frequent
request is: &ldquo;Tell us about a failed project&rdquo;. Of course, I never
fail as a data scientist, how could I? A data science task
involves a combination of domain knowledge and data, neither is
held or produced by me, and a question someone else wants an
answer to. All I do as a data scientist is encoding the domain
knowledge as a model, updating the model&rsquo;s latent variables
based on the data, and computing a quantitative answer to the
question. There are ways to <a href="http://www.stat.columbia.edu/~gelman/book/">ensure adequacy of the model, check
convergence of inference, and express uncertainty of the
answer</a>.
Just doing all these steps by the book ensures that there is
absolutely no way to fail. Consider the task of classifying
<a href="http://yann.lecun.com/exdb/mnist/">hand-written digits</a> &mdash;
although different models may have different accuracy, there is
no way to ‘fail’ as long as one does things as taught. Or is
there?</p>
<p>Let us see what a failure is. A failure is not a wrong model
choice, or poor convergence of inference, or a mistake in
computing compatibility intervals. Those are manifestations of
incompetence rather than failure. A failure happens when the
data scientist does everything right, but still causes a
disaster, hopefully small and easy to recover from. Let me
argue that a failure can only happen if the data scientist makes
a decision based on hard to validate assumptions, and those
assumptions turn out to be too far from the reality.</p>
<p>But do data scientists make any ‘voluntary’ decisions at all?
Turns out they do. If the task is label assignment, then the
decision is the compromise between precision and recall. For
forecasting, the compromise is between forecast stability and
width of confidence interval. For clustering, one has to
balance, explicitly or implicitly, between the number of
clusters and similarity of members of each cluster. Despite
apparent dissimilarities, all of these decisions are kinds of
<a href="https://towardsdatascience.com/the-exploration-exploitation-dilemma-f5622fbe1e82"><em>exploration-exploitation
compromise</em></a>.  Exploration-exploitation
compromise always addresses yet unseen data and yet undiscovered
knowledge, and thus acting by the book does not guarantee
success. Sometimes, a wrong exploration-exploitation compromise
is made, and this is how a data science project fails.</p>
<p>To conclude, an example. I took upon a task of automated traffic
acquisition &mdash; paying for visits to a web page to earn from
advertisements on that page. Visitors are acquired through an
auction, so one wants to bid higher if one anticipates higher
earnings. I deployed a model for temporal forecasting of visit
value, and a decision algorithm to choose the optimal bid given
the forecast. The algorithm accounted for forecast
uncertainty, maximized expected gain, took care of risks, and
did everything ‘right’, by the book. It worked well for a while,
but eventually &mdash; and suddenly &mdash; two different extreme cases
popped up, incurring losses (which luckily where quickly
mitigated):</p>
<ul>
<li>On a small number of campaigns, the actual visit value
suddenly dropped after steadily going up, violating smoothness
assumptions. A smoothness assumption is over-exploitation. The
result was trading at loss, for a short time but with high
traffic volume and cost.</li>
<li>On another small group of campaigns, the traffic went down
almost to zero due to a low visit value forecast, followed by
low bids due to a broad safety margin. A broad safety margin
is over-exploration, but with close to zero traffic
the forecasting ceased to be reliable, resulting in wasted
resources and lost opportunities.</li>
</ul>
<p>Both failures were fixed, eventually. What is important though
is the cause of the failures: both happened due to inadequate
exploration-exploitation assumptions introduced into the
algorithm, neither could be discovered based on either
historical data or model-based simulations.</p>

    

    


</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="/posts/how-they-fail/">How Data Scientists Fail</a>
        </li>
        
        <li>
            <a href="/posts/">Posts</a>
        </li>
        
        <li>
            <a href="/posts/double-speed-replay/">Double Speed Replay</a>
        </li>
        
        <li>
            <a href="/posts/h2typ/">How To Train Your Program</a>
        </li>
        
        <li>
            <a href="/posts/stochastic-conditioning/">Stochastic conditioning</a>
        </li>
        
    </ol>
</section>

    
    
        <section>
    
        
    
        
    
</section>
    
</aside>

      </div>
    </div>
    

    
      






<footer class="blog-footer w-100">
    <nav class="navbar navbar-light bg-light">
        <p class="w-100 text-center">Hugo template made with ❤ by <a href="https://github.com/Xzya">Xzya</a>, inspired by <a href="https://github.com/alanorth/hugo-theme-bootstrap4-blog">hugo-theme-bootstrap4-blog</a></p>
        <p class="w-100 text-center"><a href="#">Back to top</a></p>
    </nav>
</footer>

    

    
    
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>

    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
    <script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
  </body>
</html>
