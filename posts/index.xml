<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Offtopia — nothing personal</title>
    <link>http://dtolpin.github.io/posts/</link>
    <description>Recent content in Posts on Offtopia — nothing personal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Aug 2021 15:17:00 +0300</lastBuildDate><atom:link href="http://dtolpin.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Stochastic conditioning</title>
      <link>http://dtolpin.github.io/posts/stochastic-conditioning/</link>
      <pubDate>Mon, 09 Aug 2021 15:17:00 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/stochastic-conditioning/</guid>
      <description>Probabilistic programs implement statistical models. Commonly, probabilistic programs follow the Bayesian generative pattern:
\begin{equation} \begin{aligned} x &amp;amp; \sim \mathrm{Prior} \\
y &amp;amp; \sim \mathrm{Conditonal}(x) \end{aligned} \end{equation}
 A prior is imposed on the latent variable $x$. Then, observations $y$ are drawn from a distribution conditioned on $x$.  The program and the observations are passed to an inference algorithm which infers the posterior of latent variable $x$.
The questions is: what is observed?</description>
    </item>
    
    <item>
      <title>BDA Model Too Tough for Stan</title>
      <link>http://dtolpin.github.io/posts/nypopu/</link>
      <pubDate>Sat, 29 Aug 2020 18:00:54 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/nypopu/</guid>
      <description>I taught a course on Bayesian data analysis, closely following the book by Andrew Gelman et al., but with the twist of using probabilistic programming, either Stan or Infergo, for all examples and exercises. However, it turned out that at least one important problem in the book is beyond the capabilities of Stan.
This case study is inspired by Section 7.6 in Bayesian Data Analysis, originally a paper published in 1983 by Ronald Rubin.</description>
    </item>
    
    <item>
      <title>There Are No Outliers</title>
      <link>http://dtolpin.github.io/posts/weighted-white/</link>
      <pubDate>Mon, 03 Jun 2019 17:39:28 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/weighted-white/</guid>
      <description>Gaussian processes are great for time series forecasting. The time series does not have to be regular &amp;mdash; &amp;lsquo;missing data&amp;rsquo; is not an issue. A kernel can be chosen to express trend, seasonality, various degrees of smoothness, non-stationarity. External predictors can be added as input dimensions. A prior can be chosen to provide a reasonable forecast when little or even no data is available.
However, behind the Gaussian process stands an assumption that all observations come from a Gaussian distribution with constant noise and the mean lying on a smooth function of time.</description>
    </item>
    
    <item>
      <title>A Go Transgression</title>
      <link>http://dtolpin.github.io/posts/goid/</link>
      <pubDate>Tue, 02 Apr 2019 10:19:13 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/goid/</guid>
      <description>Go gives the programmer introspection into every aspect of the language, and of a running program. But to one thing the programmer does not have access, and it is the goroutine identifier. Because the day the programmers know the goroutine identifier, they create goroutine-local storage through shared access and mutexes, and shall surely die.
In Infergo, I had to have goroutine-local storage. Here is how I got efficient goroutine-local storage in Go.</description>
    </item>
    
    <item>
      <title>Go Programs That Learn</title>
      <link>http://dtolpin.github.io/posts/infergo/</link>
      <pubDate>Wed, 28 Nov 2018 18:19:13 +0200</pubDate>
      
      <guid>http://dtolpin.github.io/posts/infergo/</guid>
      <description>There are so many probabilistic programming languages that it is hard to choose one. Because it is so hard to choose one, a probabilistic programmer has two options:
 invent a new probabilistic programming language, or write probabilistic programs in a regular programming language.  The former choice is easier to make, that&amp;rsquo;s why there are so many different probabilistic programming languages. But writing programs is so much easier in a regular language, and programs in regular languages can do many useful things.</description>
    </item>
    
    <item>
      <title>A Small Program Can Be a Big Challenge</title>
      <link>http://dtolpin.github.io/posts/session-depth/</link>
      <pubDate>Wed, 15 Aug 2018 22:49:53 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/session-depth/</guid>
      <description>[Poster: html, pdf]
A good part of today&amp;rsquo;s internet content is created and shaped for delivering advertisements. Internet pages are interconnected by links, and a visitor is likely to open multiple pages from the same publisher. After a while, visitors leave the web site, either due to clicking on an advertisement or just because they get bored and switch to other content or activity.
The probability distribution of the session depth — the number of pages opened during a single visit — is an important metric for the publisher.</description>
    </item>
    
    <item>
      <title>How to Hug a Data Scientist</title>
      <link>http://dtolpin.github.io/posts/how-to-hug/</link>
      <pubDate>Mon, 09 Apr 2018 14:19:13 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/how-to-hug/</guid>
      <description>Sometimes, a data scientist is the first engineer in a software project. More often though a data scientist joins the team when there is working code, ready for deploying or even deployed. Here is how the latter case rolls out:
 We write a piece of software. Thanks to continous delivery, we fix our bugs quickly and release new improved versions on time. Our code is fully tested, easy to change, and pieces fit each other smoothly.</description>
    </item>
    
    <item>
      <title>On Brain Teasers at Job Interviews</title>
      <link>http://dtolpin.github.io/posts/on-brain-teasers/</link>
      <pubDate>Mon, 26 Jun 2017 13:08:07 +0300</pubDate>
      
      <guid>http://dtolpin.github.io/posts/on-brain-teasers/</guid>
      <description>I went to a few job interviews during past weeks. Most interviewers asked me to tell about problems I had solved, and to suggest a solution to a problem they really needed to solve. Some though offered me to solve brain teasers — problems they (or others) invented to test candidates. I solved most, but I felt bad about it. I can imagine many bright candidates who would fail an interview because of brain teasers.</description>
    </item>
    
  </channel>
</rss>
